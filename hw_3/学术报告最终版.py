#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# === ç½‘ç»œè¯·æ±‚ä¸ç½‘é¡µè§£æ ===
import requests
from bs4 import BeautifulSoup
import re
import csv

# === æ•°æ®å¤„ç†ä¸åˆ†æ ===
import pandas as pd
import numpy as np
from collections import Counter
from itertools import tee, islice
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from sklearn.linear_model import LinearRegression

# === å¯è§†åŒ– ===
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
from wordcloud import WordCloud

# === å…¶ä»–å·¥å…· ===
import string


# In[2]:


def fix_conference_spacing(name):
    # ä¿®æ­£ä¼šè®®åç§°ä¸­çš„ç©ºæ ¼é—®é¢˜
    name = re.sub(r'(\d+(?:st|nd|rd|th))([A-Z])', r'\1 \2', name)
    name = re.sub(r'([A-Za-z])(\d{4})', r'\1 \2', name)
    return name

def get_html(url):
    print(f"æ­£åœ¨æŠ“å–é¡µé¢: {url}")
    r = requests.get(url)
    if r.status_code != 200:
        print(f"æŠ“å–å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{r.status_code}ï¼Œè·³è¿‡è¯¥é¡µ")
        return None
    return r.text


# In[3]:


def parse_papers_from_year_page(html, year, conf_name):
    soup = BeautifulSoup(html, 'html.parser')
    results = []

    entries = soup.find_all('li', class_='entry inproceedings')

    for entry in entries:
        title_tag = entry.find('span', class_='title')
        title = title_tag.text.strip() if title_tag else ""

        authors = []
        for author_tag in entry.find_all('span', itemprop='author'):
            name_tag = author_tag.find('span', itemprop='name')
            if name_tag:
                authors.append(name_tag.text.strip())

        conference_name = f"{conf_name.upper()} {year}"
        conference_name = fix_conference_spacing(conference_name)

        link_tag = entry.find('a', href=True)
        link = link_tag['href'] if link_tag else ""

        results.append({
            'title': title,
            'authors': authors,
            'year': str(year),
            'conference': conference_name,
            'link': link
        })

    return results


# In[4]:


def crawl_conference_years(conf_name, start_year=2020, end_year=2025):
    all_papers = []
    for year in range(end_year, start_year - 1, -1):
        url = f"https://dblp.uni-trier.de/db/conf/{conf_name}/{conf_name}{year}.html"
        html = get_html(url)
        if html:
            papers = parse_papers_from_year_page(html, year, conf_name)
            all_papers.extend(papers)
    return all_papers


# In[5]:


# æµ‹è¯•æŠ“å–å¤šå¹´ä»½æ•°æ®
papers_ijcai = crawl_conference_years('ijcai', 2020, 2025)
papers_cvpr = crawl_conference_years('cvpr', 2020, 2025)
papers_aaai = crawl_conference_years('aaai', 2020, 2025)


# In[ ]:


papers_ijcai[:3]  # æ˜¾ç¤ºå‰3æ¡


# In[ ]:


papers_cvpr[:3]  # æ˜¾ç¤ºå‰3æ¡


# In[ ]:


papers_aaai[:3]  # æ˜¾ç¤ºå‰3æ¡


# In[6]:


def save_papers_to_csv(papers, filename="papers.csv"):
    # å®šä¹‰ CSV æ–‡ä»¶çš„å­—æ®µåï¼ˆåˆ—åï¼‰
    fieldnames = ['title', 'authors', 'year', 'conference', 'link']

    with open(filename, mode='w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)

        # å†™å…¥è¡¨å¤´
        writer.writeheader()

        for paper in papers:
            # authors åˆ—æ˜¯åˆ—è¡¨ï¼Œè½¬æˆç”¨åˆ†å·åˆ†éš”çš„å­—ç¬¦ä¸²
            paper['authors'] = '; '.join(paper['authors'])
            writer.writerow(paper)

    print(f"æˆåŠŸä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ°æ–‡ä»¶ï¼š{filename}")


# In[7]:


save_papers_to_csv(papers_ijcai, "ijcai_papers_2020_2025.csv")
save_papers_to_csv(papers_cvpr, "cvpr_papers_2020_2025.csv")
save_papers_to_csv(papers_aaai, "aaai_papers_2020_2025.csv")


# In[23]:


def plot_year_trend_from_csv(csv_file, chart_title="ä¼šè®®è®ºæ–‡æ•°é‡å¹´åº¦å˜åŒ–è¶‹åŠ¿"):
    """
    ä» CSV æ–‡ä»¶è¯»å–è®ºæ–‡æ•°æ®ï¼Œå¹¶ç»˜åˆ¶æŒ‰å¹´ä»½ç»Ÿè®¡çš„è®ºæ–‡æ•°é‡è¶‹åŠ¿å›¾ã€‚

    :param csv_file: strï¼ŒCSV æ–‡ä»¶è·¯å¾„
    :param chart_title: strï¼Œå›¾è¡¨æ ‡é¢˜ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
    """
    df = pd.read_csv(csv_file)

    # å¤„ç†å¹´ä»½å­—æ®µ
    df['year'] = pd.to_numeric(df['year'], errors='coerce')
    df = df.dropna(subset=['year'])
    df['year'] = df['year'].astype(int)

    # æŒ‰å¹´ä»½ç»Ÿè®¡æ•°é‡
    year_counts = df['year'].value_counts().sort_index()
    plot_df = pd.DataFrame({
        'year': year_counts.index,
        'count': year_counts.values
    })

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    rcParams['font.sans-serif'] = ['SimSun']  # ä¸­æ–‡æ˜¾ç¤º
    rcParams['axes.unicode_minus'] = False

    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    sns.lineplot(data=plot_df, x='year', y='count', marker='o')
    plt.xlabel("å¹´ä»½")
    plt.ylabel("è®ºæ–‡æ•°é‡")
    plt.title(chart_title)
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(chart_title)
    plt.show()
    plt.close()


# In[24]:


plot_year_trend_from_csv("ijcai_papers_2020_2025.csv",chart_title="IJCAI 2020-2025 å¹´è®ºæ–‡æ•°é‡å˜åŒ–è¶‹åŠ¿")
plot_year_trend_from_csv("cvpr_papers_2020_2025.csv",chart_title="CVPR 2020-2025 å¹´è®ºæ–‡æ•°é‡å˜åŒ–è¶‹åŠ¿")
plot_year_trend_from_csv("aaai_papers_2020_2025.csv",chart_title="AAAI 2020-2025 å¹´è®ºæ–‡æ•°é‡å˜åŒ–è¶‹åŠ¿")


# In[25]:


# åœç”¨è¯é›†åˆï¼ˆè‹±æ–‡+è‡ªå®šä¹‰ï¼‰
stop_words = set(ENGLISH_STOP_WORDS)
custom_stopwords = {
    'february'
}
stop_words = stop_words.union(custom_stopwords)

# ç”Ÿæˆnå…ƒçŸ­è¯­ï¼ˆngramï¼‰
def generate_ngrams(words, n):
    iters = tee(words, n)
    for i, it in enumerate(iters):
        next(islice(it, i, i), None)
    return zip(*iters)

def generate_bigram_wordcloud(papers, start_year, end_year, title, top_k=100):
    # è¿‡æ»¤å‡ºç›®æ ‡å¹´ä»½èŒƒå›´å†…è®ºæ–‡æ ‡é¢˜
    titles = [paper['title'] for paper in papers
              if paper.get('year') and str(paper['year']).isdigit()
              and start_year <= int(paper['year']) <= end_year]

    all_text = ' '.join(titles).lower()
    all_text = all_text.translate(str.maketrans('', '', string.punctuation))

    words = [w for w in all_text.split() if w not in stop_words and len(w) > 2]

    bigrams = [' '.join(bi) for bi in generate_ngrams(words, 2)]
    bigram_counts = Counter(bigrams)

    # å»é™¤æ— æ•ˆçŸ­è¯­
    exclude_phrases = {"student abstract", "call for", "revised selected"}
    for phrase in exclude_phrases:
        if phrase in bigram_counts:
            del bigram_counts[phrase]

    print(f"Top 10 bigrams ({start_year}-{end_year}):")
    for phrase, freq in bigram_counts.most_common(10):
        print(f"{phrase}: {freq}")

    wc = WordCloud(width=1000, height=600, background_color='white', colormap='plasma')
    wc.generate_from_frequencies(dict(bigram_counts.most_common(top_k)))

    plt.figure(figsize=(12, 8))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=18)
    plt.tight_layout()
    plt.savefig(title)
    plt.show()
    plt.close()


# In[26]:


# å®šä¹‰è¦åˆå¹¶çš„CSVæ–‡ä»¶è·¯å¾„
csv_files = [
    "aaai_papers_2020_2025.csv",
    "cvpr_papers_2020_2025.csv",
    "ijcai_papers_2020_2025.csv"
]

# è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶
dfs = [pd.read_csv(f) for f in csv_files]
combined_df = pd.concat(dfs, ignore_index=True)
# ä¿å­˜åˆ°æ–°æ–‡ä»¶
combined_df.to_csv("all_conference_papers_2020_2025.csv", index=False, encoding='utf-8')
print("âœ… å·²ä¿å­˜ä¸º all_conference_papers_2020_2025.csv")


# In[27]:


def generate_wordcloud_from_csv(csv_path, start_year=2020, end_year=2025):
    # ç¡®ä¿ generate_bigram_wordcloud å‡½æ•°å·²ç»å®šä¹‰

    df = pd.read_csv(csv_path)
    papers = df.to_dict(orient='records')

    # ç”Ÿæˆè¯äº‘
    generate_bigram_wordcloud(
        papers, 
        start_year=start_year, 
        end_year=end_year, 
        title=f'{start_year}-{end_year} é«˜é¢‘åŒè¯è¯äº‘'
    )


# In[28]:


generate_wordcloud_from_csv('all_conference_papers_2020_2025.csv', start_year=2020, end_year=2022)
generate_wordcloud_from_csv('all_conference_papers_2020_2025.csv', start_year=2023, end_year=2025)


# In[29]:


def predict_and_plot_by_year(csv_path, title="ä¼šè®®è®ºæ–‡æ•°é‡è¶‹åŠ¿ï¼ˆæŒ‰å¹´ä»½ï¼‰"):
    df = pd.read_csv(csv_path)

    # ä»…ä¿ç•™æœ‰æ•ˆå¹´ä»½
    df = df[df['year'].apply(lambda x: str(x).isdigit())]
    df['year'] = df['year'].astype(int)

    # ç»Ÿè®¡æ¯å¹´è®ºæ–‡æ•°é‡
    year_counts = df['year'].value_counts().sort_index()
    years = list(year_counts.index)
    counts = list(year_counts.values)

    # è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
    X = np.array(years).reshape(-1, 1)
    y = np.array(counts)
    model = LinearRegression()
    model.fit(X, y)

    # é¢„æµ‹ä¸‹ä¸€å¹´
    next_year = max(years) + 1
    pred = model.predict([[next_year]])[0]

    print("ğŸ“˜ é¢„æµ‹ç»“æœï¼š")
    print(f"é¢„æµ‹ {next_year} å¹´è®ºæ–‡æ•°é‡ï¼š{pred:.2f} ç¯‡")

    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    plt.scatter(years, counts, color='blue', label='å®é™…è®ºæ–‡æ•°é‡')
    plt.plot(years, model.predict(X), color='green', linestyle='--', label='çº¿æ€§æ‹Ÿåˆ')
    plt.scatter([next_year], [pred], color='red', label=f'é¢„æµ‹ {next_year}')
    plt.xlabel("å¹´ä»½")
    plt.ylabel("è®ºæ–‡æ•°é‡")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(title)
    plt.show()
    plt.close()
    return next_year, pred


# In[33]:


predict_and_plot_by_year("ijcai_papers_2020_2025.csv", title="IJCAI 2020-2025 è®ºæ–‡æ•°é‡è¶‹åŠ¿é¢„æµ‹")


# In[34]:


predict_and_plot_by_year("cvpr_papers_2020_2025.csv", title="CVPR 2020-2025 è®ºæ–‡æ•°é‡è¶‹åŠ¿é¢„æµ‹")


# In[36]:


predict_and_plot_by_year("aaai_papers_2020_2025.csv", title="AAAI 2020-2025 è®ºæ–‡æ•°é‡è¶‹åŠ¿é¢„æµ‹")


# In[ ]:




